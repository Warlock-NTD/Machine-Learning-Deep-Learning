{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bp.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NjJHeEAQb2Iu",
        "LVica4Uxb5Vu",
        "MXa-1tHl-5Ms",
        "rR6tCFROb9oX",
        "S5t50kCacAI3",
        "pfm1j_Ui8Z9o",
        "HHz_bLwiQUpS",
        "uZdSPNC0Q5bZ",
        "4tzBj0uq8h4m",
        "NsAnUUScL2C9",
        "BRt5TIlU8yUd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b6e11c9203f74ba18f2e4f9091047172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b79ca4e00bd34b748653fb7c59d98d6b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_62a966ce692649fda9bf7f982096e495",
              "IPY_MODEL_7f7bf0a86d6647a796afc8d28c4ac34b",
              "IPY_MODEL_76e282f7bf9b42568b0138dd528574c7"
            ]
          }
        },
        "b79ca4e00bd34b748653fb7c59d98d6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62a966ce692649fda9bf7f982096e495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3f4911c7472749b0b56517d7da41474b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "epoch 199 step 38 train_loss 0.0546                train_acc 98.92% val_acc 97.14: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10c43ac8a49342fdadcd0c99640627ac"
          }
        },
        "7f7bf0a86d6647a796afc8d28c4ac34b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_909d0376782b47ac9ebed846741a138e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 200,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0da3b9e8ad9a4198a8dbffa0f6d2e690"
          }
        },
        "76e282f7bf9b42568b0138dd528574c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_574975197555480b85990b6c9ad721c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 200/200 [01:58&lt;00:00,  1.62it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c43fb1e9de5d4dce9117150868cd280c"
          }
        },
        "3f4911c7472749b0b56517d7da41474b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10c43ac8a49342fdadcd0c99640627ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "909d0376782b47ac9ebed846741a138e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0da3b9e8ad9a4198a8dbffa0f6d2e690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "574975197555480b85990b6c9ad721c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c43fb1e9de5d4dce9117150868cd280c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu84Hh1I3hVY"
      },
      "source": [
        "import numpy as np\n",
        "import unittest\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-YpYDJ8ROdR"
      },
      "source": [
        "# The unit tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjJHeEAQb2Iu"
      },
      "source": [
        "## Tests for Fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EanSBThMPKMK"
      },
      "source": [
        "class TestFCMethods(unittest.TestCase):\n",
        "  def test_fc_init(self):\n",
        "    fc = FC(n_in=10, n_out=5, activation=\"sigmoid\")\n",
        "    self.assertEqual(fc.n_in, 10)\n",
        "    self.assertEqual(fc.n_out, 5)\n",
        "    self.assertEqual(fc.W.shape, (10,5))\n",
        "    self.assertEqual(fc.dW.shape, (10,5))\n",
        "    self.assertEqual(fc.activation, \"sigmoid\")\n",
        "\n",
        "  def test_fc_forward(self):\n",
        "    fc = FC(n_in=10, n_out=5, activation=\"sigmoid\")\n",
        "    x = np.zeros((3, 10), dtype=np.float32)\n",
        "    y = fc.forward(x)\n",
        "    error = np.sum(np.abs(y-np.ones((3,5))*0.5))\n",
        "    self.assertEqual(y.shape, (3, 5))\n",
        "    self.assertLess(error, 1e-6)\n",
        "\n",
        "  def test_fc_forward_identity(self):\n",
        "    fc = FC(n_in=10, n_out=5, activation=None)\n",
        "    x = np.zeros((3, 10), dtype=np.float32)\n",
        "    y = fc.forward(x)\n",
        "    error = np.sum(np.abs(y-np.zeros((3,5))))\n",
        "    self.assertEqual(y.shape, (3, 5))\n",
        "    self.assertLess(error, 1e-6)\n",
        "\n",
        "  def test_fc_backward_identity(self):\n",
        "    fc = FC(n_in=10, n_out=5, activation=None)\n",
        "    x = np.zeros((3, 10), dtype=np.float32)\n",
        "    y = fc.forward(x)\n",
        "    dx = fc.backward(np.ones_like(y))\n",
        "    ## exercise: should add test on error of dx here\n",
        "    self.assertEqual(dx.shape, x.shape)\n",
        "    self.assertEqual(fc.dW.shape, fc.W.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVica4Uxb5Vu"
      },
      "source": [
        "## Tests for MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI-o9ORkPOBv"
      },
      "source": [
        "class TestMLPMethods(unittest.TestCase):\n",
        "  def test_mlp_init(self):\n",
        "    model = MLP(n_in=10, hiddens=[5, 2])\n",
        "    layer0 = model.layers[0]\n",
        "    layer1 = model.layers[1]\n",
        "    self.assertEqual(model.n_in, 10)\n",
        "    self.assertEqual(model.hiddens, [5, 2])\n",
        "    self.assertEqual(layer0.n_in, 10)\n",
        "    self.assertEqual(layer0.n_out, 5)\n",
        "    self.assertEqual(layer1.n_in, 5)\n",
        "    self.assertEqual(layer1.n_out, 2)\n",
        "\n",
        "  def test_mlp_forward(self):\n",
        "    model = MLP(n_in=10, hiddens=[5, 2])\n",
        "    x = np.zeros((3, 10), dtype=np.float32)\n",
        "    y = model.forward(x)\n",
        "    self.assertEqual(y.shape, (3, 2))\n",
        "\n",
        "  def test_mlp_backward(self):\n",
        "    model = MLP(n_in=10, hiddens=[5, 2])\n",
        "    x = np.zeros((3, 10), dtype=np.float32)\n",
        "    y = model.forward(x)\n",
        "    dx = model.backward(np.ones_like(y))\n",
        "    self.assertEqual(dx.shape, x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXa-1tHl-5Ms"
      },
      "source": [
        "## Tests for ResBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTZeMt_G-i77"
      },
      "source": [
        "class TestResBlockMethods(unittest.TestCase):\n",
        "  def test_res_init(self):\n",
        "    model = ResidualBlock(n_in=10, hiddens=[5, 6, 2])\n",
        "    fc = model.input_fc\n",
        "    layer0 = model.block.layers[0]\n",
        "    layer1 = model.block.layers[1]\n",
        "    skip = model.skip\n",
        "    self.assertEqual((fc.n_in, fc.n_out), (10,5))\n",
        "    self.assertEqual(model.block.n_in, 5)\n",
        "    self.assertEqual(model.block.hiddens, [6, 2])\n",
        "    self.assertEqual(layer0.n_in, 5)\n",
        "    self.assertEqual(layer0.n_out, 6)\n",
        "    self.assertEqual(layer1.n_in, 6)\n",
        "    self.assertEqual(layer1.n_out, 2)\n",
        "    self.assertEqual(skip.n_in, 5)\n",
        "    self.assertEqual(skip.n_out, 2)\n",
        "\n",
        "  def test_res_identity(self):\n",
        "    model = ResidualBlock(n_in=10, hiddens=[5, 6, 5])\n",
        "    fc = model.input_fc\n",
        "    layer0 = model.block.layers[0]\n",
        "    layer1 = model.block.layers[1]\n",
        "    skip = model.skip\n",
        "    self.assertEqual((fc.n_in, fc.n_out), (10,5))\n",
        "    self.assertEqual(model.block.n_in, 5)\n",
        "    self.assertEqual(model.block.hiddens, [6, 5])\n",
        "    self.assertEqual(layer0.n_in, 5)\n",
        "    self.assertEqual(layer0.n_out, 6)\n",
        "    self.assertEqual(layer1.n_in, 6)\n",
        "    self.assertEqual(layer1.n_out, 5)\n",
        "    self.assertIsNone(skip, None)\n",
        "\n",
        "  def test_res_forward(self):\n",
        "    model = ResidualBlock(n_in=10, hiddens=[5, 2])\n",
        "    x = np.zeros((3, 10), dtype=np.float32)\n",
        "    y = model.forward(x)\n",
        "    self.assertEqual(y.shape, (3, 2))\n",
        "\n",
        "  def test_res_backward(self):\n",
        "    model = ResidualBlock(n_in=10, hiddens=[5, 2])\n",
        "    x = np.zeros((3, 10), dtype=np.float32)\n",
        "    y = model.forward(x)\n",
        "    dx = model.backward(np.ones_like(y))\n",
        "    self.assertEqual(dx.shape, x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR6tCFROb9oX"
      },
      "source": [
        "## Tests for Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SptdBfy-Vzeb"
      },
      "source": [
        "class TestCEMethods(unittest.TestCase):\n",
        "  def test_ce_forward(self):\n",
        "    ypred = np.zeros((10, 5))\n",
        "    ytrue = np.array([0,1,2,3,4,0,1,2,3,4], dtype=int)\n",
        "    ce = CrossEntropyLoss()\n",
        "    loss = ce.forward(ypred, ytrue)\n",
        "    self.assertAlmostEqual(loss, -10*math.log(1/5))\n",
        "\n",
        "  def test_ce_backward(self):\n",
        "    ypred = np.zeros((10, 5))\n",
        "    ytrue = np.array([0,1,2,3,4,0,1,2,3,4], dtype=int)\n",
        "    ce = CrossEntropyLoss()\n",
        "    loss = ce.forward(ypred, ytrue)\n",
        "    d_ypred = ce.backward()\n",
        "    desired = np.ones((10,5))*0.2\n",
        "    desired[range(10), ytrue] -= 1\n",
        "    error = np.sum(np.abs(d_ypred-desired))\n",
        "    self.assertAlmostEqual(error, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5t50kCacAI3"
      },
      "source": [
        "## Tests for Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6ofVdrLQOGN"
      },
      "source": [
        "class TestSGDMethods(unittest.TestCase):\n",
        "  def test_sgd_init(self):\n",
        "    model = MLP(n_in=10, hiddens=[5, 2])\n",
        "    sgd = SGDOptimizer(model, learning_rate=0.2, regularization=0.1)\n",
        "    param = sgd.parameters()\n",
        "    grad = sgd.grads()\n",
        "    \n",
        "    for p, g in zip(param, grad):\n",
        "      self.assertEqual(p.shape, g.shape)\n",
        "    self.assertEqual(sgd.learning_rate, 0.2)\n",
        "    self.assertEqual(sgd.regularization, 0.1)\n",
        "\n",
        "\n",
        "  def test_sgd_zero_grad(self):\n",
        "    model = MLP(n_in=10, hiddens=[5, 2])\n",
        "    sgd = SGDOptimizer(model, learning_rate=0.2)\n",
        "    sgd.zero_grad()\n",
        "\n",
        "    for g in sgd.grads():\n",
        "      self.assertAlmostEqual(np.sum(np.abs(g)), 0)\n",
        "\n",
        "  def test_sgd_step(self):\n",
        "    model = MLP(n_in=10, hiddens=[5, 2])\n",
        "    sgd = SGDOptimizer(model, learning_rate=0.2)\n",
        "    loss_func = CrossEntropyLoss()\n",
        "    \n",
        "    x = np.zeros((3, 10), dtype=np.float32)\n",
        "    ytrue = np.array([0,1,0], dtype=int)\n",
        "\n",
        "    ypred = model.forward(x)\n",
        "    loss = loss_func.forward(ypred, ytrue)\n",
        "\n",
        "    sgd.zero_grad()\n",
        "    dout = loss_func.backward()\n",
        "    dx = model.backward(dout)\n",
        "    sgd.step()\n",
        "\n",
        "  def test_sgd_n_step(self):\n",
        "    model = MLP(n_in=10, hiddens=[5, 2])\n",
        "    sgd = SGDOptimizer(model, learning_rate=0.02)\n",
        "    loss_func = CrossEntropyLoss()\n",
        "    n_step = 10\n",
        "    \n",
        "    x = np.zeros((3, 10), dtype=np.float32)\n",
        "    ytrue = np.array([0,1,0], dtype=int)\n",
        "\n",
        "    print()\n",
        "    for step in range(n_step):\n",
        "      model.train()\n",
        "      ypred = model.forward(x)\n",
        "      loss = loss_func.forward(ypred, ytrue)\n",
        "\n",
        "      print(f\"step {step} {loss:.4f}\")\n",
        "      if step > 0:\n",
        "        self.assertLess(loss, old_loss) ## SGD step reduces loss function\n",
        "      old_loss = loss\n",
        "\n",
        "      sgd.zero_grad()\n",
        "      dout = loss_func.backward()\n",
        "      dx = model.backward(dout)\n",
        "      sgd.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfm1j_Ui8Z9o"
      },
      "source": [
        "## Tests for Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bbJmH-n8b6D"
      },
      "source": [
        "class TestDropoutMethods(unittest.TestCase):\n",
        "  def test_dropout_init(self):\n",
        "    do = Dropout(p=0.2)\n",
        "    self.assertEqual(do.p, 0.2)\n",
        "    self.assertEqual(do.parameters(), [])\n",
        "    self.assertEqual(do.grads(), [])\n",
        "\n",
        "  def test_dropout_forward(self):\n",
        "    do = Dropout(p=0.30)\n",
        "    np.random.seed(42)\n",
        "    x = np.ones((10, 10))\n",
        "    y = do.forward(x)\n",
        "    count_zero = np.sum(y==0)\n",
        "    self.assertEqual(count_zero, 34)\n",
        "\n",
        "  def test_dropout_back(self):\n",
        "    do = Dropout(p=0.30)\n",
        "    np.random.seed(42)\n",
        "    x = np.ones((10, 10))*2\n",
        "    y = do.forward(x)\n",
        "    dx = do.backward(np.ones_like(y))\n",
        "    count_zero = np.sum(dx==0)\n",
        "    self.assertEqual(count_zero, 34)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM6K_LZbdqoO"
      },
      "source": [
        "# FC Layer, MLP, ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHz_bLwiQUpS"
      },
      "source": [
        "\n",
        "## Fully connected *layer*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJOxwpwDAnvm"
      },
      "source": [
        "class FC:\n",
        "  def __init__(self, n_in, n_out, activation=None):\n",
        "    self.n_in = n_in\n",
        "    self.n_out = n_out\n",
        "    self.activation = activation\n",
        "    # self.W = np.zeros(shape=(n_in, n_out))\n",
        "    std = math.sqrt(2) if activation == \"relu\" else 1.0\n",
        "    self.W = np.random.normal(scale=1.0/math.sqrt(n_in), size=(n_in, n_out))\n",
        "    self.dW = np.zeros(shape=(n_in, n_out))\n",
        "\n",
        "  @staticmethod\n",
        "  def stable_sigmoid(x):\n",
        "    z = np.zeros_like(x)\n",
        "    z[x >= 0] = 1 / ( 1+np.exp(-x[x >= 0]) )\n",
        "    z[x < 0] = np.exp(x[x < 0])\n",
        "    z[x < 0] = z[x < 0] / ( 1+z[x < 0] )\n",
        "    return z\n",
        "\n",
        "  def __activation(self, a):\n",
        "    if self.activation is None:\n",
        "      f = a.copy() # N x n_out\n",
        "    elif self.activation == \"sigmoid\":\n",
        "      f = self.stable_sigmoid(a) # 1.0/(1+np.exp(-self.a))\n",
        "    elif self.activation == \"relu\":\n",
        "      f = a.copy()\n",
        "      f[a < 0] = 0\n",
        "    else:\n",
        "      raise NotImplementedError(f\"NotImplementedError FC.forward activation={self.activation}\")\n",
        "    return f\n",
        "\n",
        "  def __dactivation(self, df, f, a):\n",
        "    if self.activation is None:\n",
        "      da = df.copy() # N x n_out\n",
        "    elif self.activation == \"sigmoid\":\n",
        "      da = f*(1-f)*df # N x n_out\n",
        "    elif self.activation == \"relu\":\n",
        "      da = df.copy()\n",
        "      da[a < 0] = 0\n",
        "    else:\n",
        "      raise NotImplementedError(f\"NotImplementedError FC.backward activation={self.activation}\")\n",
        "    return da\n",
        "\n",
        "  def forward(self, x):\n",
        "    ## x: N x n_in\n",
        "    ## save computation for backward phase\n",
        "    self.x = x.copy()\n",
        "    self.a = np.matmul(x, self.W) # N x n_out\n",
        "    self.f = self.__activation(self.a)\n",
        "    return self.f\n",
        "\n",
        "  def backward(self, df):\n",
        "    ## df: N x n_out\n",
        "    ## use pre-compute self.x, self.a and self.f to compute dx and dW\n",
        "    da = self.__dactivation(df, self.f, self.a)\n",
        "    self.dW = np.einsum('ij,ik->jk', self.x, da) # n_in x n_out\n",
        "    self.dx = np.matmul(da, self.W.T) # N x n_in\n",
        "    self.df = df\n",
        "    self.da = da\n",
        "    return self.dx\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.W]\n",
        "\n",
        "  def grads(self):\n",
        "    return [self.dW]\n",
        "\n",
        "  def train(self):\n",
        "    pass\n",
        "  \n",
        "  def eval(self):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZdSPNC0Q5bZ"
      },
      "source": [
        "## Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGNRJ3Sp7eUG"
      },
      "source": [
        "class MLP:\n",
        "  def __init__(self, n_in, hiddens, activation=\"sigmoid\", last_layer_linear=True):\n",
        "    self.n_in = n_in\n",
        "    self.hiddens = hiddens\n",
        "    self.layers = [\n",
        "                   # use sigmoid activation for hidden layer\n",
        "                   # use linear activation for output layer\n",
        "                   FC(n_in=hiddens[i-1] if i > 0 else n_in,\n",
        "                      n_out=hiddens[i],\n",
        "                      activation=activation if (i < len(hiddens)-1) or (not last_layer_linear) else None)\n",
        "                   for i in range(len(hiddens))\n",
        "                   ]\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = x\n",
        "    for layer in self.layers:\n",
        "      out = layer.forward(out)\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    for layer in self.layers[::-1]:\n",
        "      dout = layer.backward(dout)\n",
        "    return dout\n",
        "\n",
        "  def parameters(self):\n",
        "    return sum([layer.parameters() for layer in self.layers], [])\n",
        "\n",
        "  def grads(self):\n",
        "    return sum([layer.grads() for layer in self.layers], [])\n",
        "\n",
        "  def train(self):\n",
        "    for layer in self.layers:\n",
        "      layer.train()\n",
        "\n",
        "  def eval(self):\n",
        "    for layer in self.layers:\n",
        "      layer.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tzBj0uq8h4m"
      },
      "source": [
        "## Residual Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61aJxSwe8nOu"
      },
      "source": [
        "class ResidualBlock:\n",
        "  def __init__(self, n_in, hiddens, activation=\"sigmoid\", last_layer_linear=False, dropout=-1.0):\n",
        "    ## initialize layers\n",
        "    n_out = hiddens[-1]\n",
        "    self.input_fc = FC(n_in, hiddens[0], activation=activation)\n",
        "    self.block = MLP(n_in=hiddens[0], hiddens=hiddens[1:], activation=activation, last_layer_linear=last_layer_linear)\n",
        "    self.skip = FC(n_in=hiddens[0], n_out=n_out, activation=activation) if hiddens[0] != n_out else None\n",
        "    self.dropout = Dropout(p=dropout) if dropout > 0 else None\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.input_fc.forward(x)\n",
        "    block_out = self.block.forward(x)\n",
        "    skip_out = self.skip.forward(x) if self.skip is not None else x\n",
        "    out = block_out + skip_out\n",
        "    out = self.dropout.forward(out) if self.dropout is not None else out\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout = self.dropout.backward(dout) if self.dropout is not None else dout\n",
        "    d_block_in = self.block.backward(dout)\n",
        "    d_skip_in  = self.skip.backward(dout) if self.skip is not None else dout\n",
        "    dx = d_block_in + d_skip_in\n",
        "    dx = self.input_fc.backward(dx)\n",
        "    return dx\n",
        "\n",
        "  def parameters(self):\n",
        "    return self.input_fc.parameters() + self.block.parameters() + (self.skip.parameters() if self.skip is not None else [])\n",
        "\n",
        "  def grads(self):\n",
        "    return self.input_fc.grads() + self.block.grads() + (self.skip.grads() if self.skip is not None else [])\n",
        "\n",
        "  def train(self):\n",
        "    self.input_fc.train()\n",
        "    self.block.train()\n",
        "    if self.skip is not None:\n",
        "      self.skip.train()\n",
        "    if self.dropout is not None:\n",
        "      self.dropout.train()\n",
        "\n",
        "  def eval(self):\n",
        "    self.input_fc.eval()\n",
        "    self.block.eval()\n",
        "    if self.skip is not None:\n",
        "      self.skip.eval()\n",
        "    if self.dropout is not None:\n",
        "      self.dropout.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsAnUUScL2C9"
      },
      "source": [
        "## Residual Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qWdGLk0CS2J"
      },
      "source": [
        "class ResNet:\n",
        "  def __init__(self, n_in, blocks_hiddens, n_out, activation=\"sigmoid\", dropout=None):\n",
        "    self.n_in = n_in\n",
        "    self.blocks_hiddens = blocks_hiddens\n",
        "    self.n_out = n_out\n",
        "\n",
        "    self.blocks = [\n",
        "                   ResidualBlock(\n",
        "                       n_in=blocks_hiddens[i-1][-1] if i > 0 else n_in,\n",
        "                       hiddens = blocks_hiddens[i],\n",
        "                       activation=activation,\n",
        "                       last_layer_linear=False,\n",
        "                       dropout=-1 if dropout is None else dropout[i]\n",
        "                       )\n",
        "                   for i in range(len(blocks_hiddens))\n",
        "                   ]\n",
        "    self.fc = FC(n_in=blocks_hiddens[-1][-1], n_out=n_out)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = x\n",
        "    for block in self.blocks:\n",
        "      out = block.forward(out)\n",
        "    out = self.fc.forward(out)\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout = self.fc.backward(dout)\n",
        "    for block in self.blocks[::-1]:\n",
        "      dout = block.backward(dout)\n",
        "    return dout\n",
        "\n",
        "  def parameters(self):\n",
        "    return sum([block.parameters() for block in self.blocks], []) + self.fc.parameters()\n",
        "\n",
        "  def grads(self):\n",
        "    return sum([block.grads() for block in self.blocks], []) + self.fc.grads()\n",
        "\n",
        "  def train(self):\n",
        "    for layer in self.blocks+[self.fc]:\n",
        "      layer.train()\n",
        "\n",
        "  def eval(self):\n",
        "    for layer in self.blocks+[self.fc]:\n",
        "      layer.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRt5TIlU8yUd"
      },
      "source": [
        "## Dropout layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4K0JEeK80cn"
      },
      "source": [
        "class Dropout:\n",
        "  def __init__(self, p):\n",
        "    self.p = p\n",
        "    self.is_train = True\n",
        "\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "  def grads(self):\n",
        "    return []\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.is_train:\n",
        "      self.mask = np.random.rand(*x.shape) < self.p\n",
        "      self.x = x\n",
        "      y = x.copy()\n",
        "      y[self.mask] = 0\n",
        "    else:\n",
        "      y = x * (1-self.p)\n",
        "    return y\n",
        "\n",
        "  def backward(self, dy):\n",
        "    dx = dy.copy()\n",
        "    dx[self.mask] = 0\n",
        "    return dx\n",
        "\n",
        "  def train(self):\n",
        "    self.is_train = True\n",
        "\n",
        "  def eval(self):\n",
        "    self.is_train = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_uUfQ0iQ963"
      },
      "source": [
        "# Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY_-xnnAbwoC"
      },
      "source": [
        "class CrossEntropyLoss:\n",
        "  @staticmethod\n",
        "  def stable_softmax(X):\n",
        "    exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "  def forward(self, ypred, ytrue):\n",
        "    ## ypred: N x n_out (logit values)\n",
        "    ## ytrue: N (class label: int value in 0-->n_out-1)\n",
        "    ## should return - sum_i sum_c y_ic \\log mu_ic\n",
        "    n, n_out = ypred.shape\n",
        "    self.ypred = ypred\n",
        "    self.ytrue = ytrue\n",
        "    self.mu = self.stable_softmax(ypred)\n",
        "    mu_ytrue = self.mu[range(n), ytrue]\n",
        "    mu_ytrue[mu_ytrue < 1e-8] = 1e-8\n",
        "    loss = np.sum(-np.log(mu_ytrue))\n",
        "    return loss\n",
        "\n",
        "  def backward(self):\n",
        "    ## should return d_ypred, derivative of loss on ypred\n",
        "    ## d_ypred = mu - y (one-hot encoding of ytrue)\n",
        "    n, n_out = self.ypred.shape\n",
        "    d_ypred = self.mu.copy()\n",
        "    d_ypred[range(n), self.ytrue] -= 1\n",
        "    return d_ypred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t69qUIAQRBdW"
      },
      "source": [
        "# Stochastic Gradient Descent optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCYdXS3Kb0Py"
      },
      "source": [
        "import math\n",
        "class SGDOptimizer:\n",
        "  def __init__(self, model, learning_rate, regularization=0.0):\n",
        "    self.model = model\n",
        "    self.learning_rate = learning_rate\n",
        "    self.regularization = regularization\n",
        "    self.current_step = 0\n",
        "\n",
        "  def parameters(self):\n",
        "    return self.model.parameters()\n",
        "\n",
        "  def grads(self):\n",
        "    return self.model.grads()\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for g in self.grads():\n",
        "      g.fill(0)\n",
        "\n",
        "  def step(self):\n",
        "    ## input is the derivative of loss function on the output of the model\n",
        "    ## dW has been computed by backward functions\n",
        "    ## perform a gradient step W = W - 1/sqrt(t) lambda dW\n",
        "    ## the learning rate is reduced over time for convergence\n",
        "    self.current_step += 1\n",
        "    for p, g in zip(self.parameters(), self.grads()):\n",
        "      g = self.regularization*p + g\n",
        "      g = np.clip(g, -1, 1)\n",
        "      p -= 1.0 / math.sqrt(self.current_step)*self.learning_rate*g\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8a-ybifSZws"
      },
      "source": [
        "import math\n",
        "class AdaGradOptimizer:\n",
        "  def __init__(self, model, learning_rate, regularization=0.0):\n",
        "    self.model = model\n",
        "    self.learning_rate = learning_rate\n",
        "    self.regularization = regularization\n",
        "    self.current_step = 0\n",
        "    self.sum_grad = [np.zeros_like(p) for p in model.parameters()]\n",
        "\n",
        "  def parameters(self):\n",
        "    return self.model.parameters()\n",
        "\n",
        "  def grads(self):\n",
        "    return self.model.grads()\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for g in self.grads():\n",
        "      g.fill(0)\n",
        "\n",
        "  def step(self):\n",
        "    ## input is the derivative of loss function on the output of the model\n",
        "    ## dW has been computed by backward functions\n",
        "    ## perform a scaled gradient step W = W - 1/sqrt(G+eps) lambda dW\n",
        "    ## the learning rate is reduced over time by square root of sum of gradient squares\n",
        "\n",
        "    eps = 1e-8\n",
        "    for p, g, G in zip(self.parameters(), self.grads(), self.sum_grad):\n",
        "      g = self.regularization*p + g\n",
        "      g = np.clip(g, -1, 1)\n",
        "      G += (g*g) ## sum of gradient squares until current step\n",
        "\n",
        "      p -= (self.learning_rate / np.sqrt(G+eps) * g)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eCyi0WfRF24"
      },
      "source": [
        "# Run the unit tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af8ka5CP7IDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a3be32-e184-4c5d-c4aa-d942305a7da0"
      },
      "source": [
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_ce_backward (__main__.TestCEMethods) ... ok\n",
            "test_ce_forward (__main__.TestCEMethods) ... ok\n",
            "test_dropout_back (__main__.TestDropoutMethods) ... ok\n",
            "test_dropout_forward (__main__.TestDropoutMethods) ... ok\n",
            "test_dropout_init (__main__.TestDropoutMethods) ... ok\n",
            "test_fc_backward_identity (__main__.TestFCMethods) ... ok\n",
            "test_fc_forward (__main__.TestFCMethods) ... ok\n",
            "test_fc_forward_identity (__main__.TestFCMethods) ... ok\n",
            "test_fc_init (__main__.TestFCMethods) ... ok\n",
            "test_mlp_backward (__main__.TestMLPMethods) ... ok\n",
            "test_mlp_forward (__main__.TestMLPMethods) ... ok\n",
            "test_mlp_init (__main__.TestMLPMethods) ... ok\n",
            "test_res_backward (__main__.TestResBlockMethods) ... ok\n",
            "test_res_forward (__main__.TestResBlockMethods) ... ok\n",
            "test_res_identity (__main__.TestResBlockMethods) ... ok\n",
            "test_res_init (__main__.TestResBlockMethods) ... ok\n",
            "test_sgd_init (__main__.TestSGDMethods) ... ok\n",
            "test_sgd_n_step (__main__.TestSGDMethods) ... ok\n",
            "test_sgd_step (__main__.TestSGDMethods) ... ok\n",
            "test_sgd_zero_grad (__main__.TestSGDMethods) ... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "step 0 1.9250\n",
            "step 1 1.9240\n",
            "step 2 1.9234\n",
            "step 3 1.9229\n",
            "step 4 1.9225\n",
            "step 5 1.9221\n",
            "step 6 1.9218\n",
            "step 7 1.9215\n",
            "step 8 1.9212\n",
            "step 9 1.9210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 20 tests in 0.053s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f1a1ff64510>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4rVeDzbouZk"
      },
      "source": [
        "# A training and evaluation example\n",
        "MLP network and ResNet (more layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbJTT4X6sqAk"
      },
      "source": [
        "class Accumulator:\n",
        "  def __init__(self):\n",
        "    self.total_sample = 0\n",
        "    self.key_values = {}\n",
        "\n",
        "  def __call__(self, n_sample, **kwargs):\n",
        "    for k, v in kwargs.items():\n",
        "      if k not in self.key_values:\n",
        "        self.key_values[k] = v\n",
        "      else:\n",
        "        self.key_values[k] += v\n",
        "    self.total_sample += n_sample\n",
        "\n",
        "  def mean(self, key):\n",
        "    return self.key_values[key] / self.total_sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuiQX3If46hE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508,
          "referenced_widgets": [
            "b6e11c9203f74ba18f2e4f9091047172",
            "b79ca4e00bd34b748653fb7c59d98d6b",
            "62a966ce692649fda9bf7f982096e495",
            "7f7bf0a86d6647a796afc8d28c4ac34b",
            "76e282f7bf9b42568b0138dd528574c7",
            "3f4911c7472749b0b56517d7da41474b",
            "10c43ac8a49342fdadcd0c99640627ac",
            "909d0376782b47ac9ebed846741a138e",
            "0da3b9e8ad9a4198a8dbffa0f6d2e690",
            "574975197555480b85990b6c9ad721c4",
            "c43fb1e9de5d4dce9117150868cd280c"
          ]
        },
        "outputId": "391d0952-6664-4d0a-a766-e130eecd3d72"
      },
      "source": [
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "from tqdm.notebook import tqdm_notebook as tqdm\n",
        "\n",
        "def prepare_data():\n",
        "  X, y = datasets.load_digits(return_X_y=True)\n",
        "  X, Xtest, y, ytest = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "  transform = preprocessing.MinMaxScaler()\n",
        "  X = transform.fit_transform(X)\n",
        "  Xtest = transform.transform(Xtest)\n",
        "  return X, Xtest, y, ytest\n",
        "\n",
        "def prepare_trainer(model):\n",
        "  # sgd = SGDOptimizer(model, learning_rate=0.1, regularization=0.03)\n",
        "  sgd = AdaGradOptimizer(model, learning_rate=0.1, regularization=0.03)\n",
        "  loss_func = CrossEntropyLoss()\n",
        "  return sgd, loss_func\n",
        "\n",
        "def prepare_data_loader(X, y, batch_size):\n",
        "  n = X.shape[0]\n",
        "  permutation = np.random.permutation(n)\n",
        "  for i in range(0, n, batch_size):\n",
        "    j = i+batch_size if i+batch_size <= n else n\n",
        "    batch_x = X[permutation[i:j]]\n",
        "    batch_y = y[permutation[i:j]]\n",
        "    yield batch_x, batch_y\n",
        "\n",
        "def get_model(n_in, n_out):\n",
        "  np.random.seed(101)\n",
        "  # model = MLP(n_in=n_in, hiddens=[128, 64, 10], activation=\"relu\")\n",
        "  # model = ResNet(n_in=n_in, blocks_hiddens=[[128,64,128]], n_out=n_out, activation=\"sigmoid\")\n",
        "  model = ResNet(\n",
        "      n_in=n_in,\n",
        "      blocks_hiddens=[[128,32,256],[64,32,64], [32,16,32], [16,8,16]],\n",
        "      n_out=n_out,\n",
        "      activation=\"relu\",\n",
        "      dropout=[0.2,0.2,0.2,0.2])\n",
        "  # model = ResNet(n_in=n_in, blocks_hiddens=[[128,16,128], [64,8,64], [32,4,32], [16,2,16]], n_out=n_out, activation=\"relu\")\n",
        "  return model\n",
        "\n",
        "class Config:\n",
        "  n_epoch = 200\n",
        "  batch_size = 32\n",
        "\n",
        "def main():\n",
        "  X, Xtest, y, ytest = prepare_data()\n",
        "  config = Config()\n",
        "\n",
        "  model = get_model(n_in=X.shape[1], n_out=10)\n",
        "  sgd, loss_func = prepare_trainer(model)\n",
        "\n",
        "  pbar = tqdm(range(config.n_epoch))\n",
        "  val_acc = 0\n",
        "  for epoch in pbar:\n",
        "    data_loader = prepare_data_loader(X, y, config.batch_size)\n",
        "\n",
        "    model.train() ## set train mode\n",
        "    accumulator = Accumulator()\n",
        "    for step, (batch_x, batch_y) in enumerate(data_loader):\n",
        "      ## forward pass\n",
        "      batch_yp = model.forward(batch_x)\n",
        "      loss = loss_func.forward(batch_yp, batch_y)\n",
        "\n",
        "      ## backward pass and an optimization step\n",
        "      sgd.zero_grad()\n",
        "      dout = loss_func.backward()\n",
        "      dx = model.backward(dout)\n",
        "      sgd.step()\n",
        "\n",
        "      ## log training progress\n",
        "      n_correct = np.sum(np.argmax(batch_yp, axis=1) == batch_y)\n",
        "      accumulator(len(batch_y), correct=n_correct, loss=loss)\n",
        "    \n",
        "      pbar.set_description(f\"epoch {epoch} step {step+1} train_loss {accumulator.mean('loss'):.4f}\\\n",
        "                train_acc {accumulator.mean('correct')*100:.2f}% val_acc {val_acc*100:.2f}\")\n",
        "\n",
        "    model.eval()  ## set evaluation mode\n",
        "    val_acc = np.sum(np.argmax(model.forward(Xtest), axis=1) == ytest) / len(ytest)\n",
        "\n",
        "  ypred = np.argmax(model.forward(Xtest), axis=1)\n",
        "  print(metrics.classification_report(ytest, ypred))\n",
        "  print(metrics.confusion_matrix(ytest, ypred))\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6e11c9203f74ba18f2e4f9091047172",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99        55\n",
            "           1       0.95      0.96      0.95        55\n",
            "           2       0.96      1.00      0.98        52\n",
            "           3       0.98      0.98      0.98        56\n",
            "           4       1.00      1.00      1.00        64\n",
            "           5       0.96      0.97      0.97        73\n",
            "           6       0.98      0.98      0.98        57\n",
            "           7       1.00      0.98      0.99        62\n",
            "           8       0.94      0.90      0.92        52\n",
            "           9       0.98      0.96      0.97        68\n",
            "\n",
            "    accuracy                           0.97       594\n",
            "   macro avg       0.97      0.97      0.97       594\n",
            "weighted avg       0.97      0.97      0.97       594\n",
            "\n",
            "[[55  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 53  1  0  0  0  0  0  1  0]\n",
            " [ 0  0 52  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 55  0  1  0  0  0  0]\n",
            " [ 0  0  0  0 64  0  0  0  0  0]\n",
            " [ 1  0  0  0  0 71  1  0  0  0]\n",
            " [ 0  0  0  0  0  1 56  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 61  0  1]\n",
            " [ 0  3  1  0  0  1  0  0 47  0]\n",
            " [ 0  0  0  1  0  0  0  0  2 65]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfdMu7mPJOHN"
      },
      "source": [
        "## pytorch_lightning\n",
        "- model\n",
        "- data loader\n",
        "- trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtO6A5oy83SD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}